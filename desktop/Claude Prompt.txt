Claude Prompt (Final â€“ CSV 1 only used in training, not in query):

Part 1 â€“ Thesis Context

We are working on our thesis titled â€œEnhancing Literary Analysis of Rizalâ€™s Novels Through Topic Modeling: A BERT-Based Approach to Thematic Search Implementation for Noli Me Tangere and El Filibusterismo.â€

We have the buod (summary) versions of Noli Me Tangere and El Filibusterismo in digital form.

Our main goal is to build a BERT-based thematic exploration and search system.

The system must:

Accept both Tagalog and English queries.

Recognize deep/archaic Tagalog words (handled via CSV 1 during training).

Leverage BERTâ€™s embeddings to naturally connect synonyms and related concepts (e.g., searching â€œedukasyonâ€ also retrieves chapters about â€œpag-aaralâ€).

Return ranked chapters thematically related to the query (e.g., searching â€œtinolaâ€ retrieves Padre Damasoâ€™s chapter).

Generate a short explanatory paragraph for each result.

Work fully offline (no API/internet).

Important: CSV 1 is only used during training to normalize deep â†’ modern â†’ English words. At runtime (Code 2), queries should not rely on CSV 1. Instead, they should be processed by the trained model directly, since it already â€œlearnedâ€ the mappings during training.

Training will be done in Google Colab with GPU enabled. Please also recommend a reasonable number of epochs (e.g., 3â€“5) for efficient training.

Part 2 â€“ CSV Formats (Fixed)

CSV 1 â€“ Deep Tagalog Words Dictionary
Columns (in order):

book_title â†’ either â€œNoli Me Tangereâ€ or â€œEl Filibusterismoâ€

deep_word â†’ the old/archaic Tagalog word

modern_word â†’ its modern Filipino equivalent

english_equivalent â†’ its English translation

CSV 2 â€“ Noli & El Fili Corpus
Columns (in order):

book_title â†’ either â€œNoli Me Tangereâ€ or â€œEl Filibusterismoâ€

chapter_number â†’ chapter number in the book

chapter_title â†’ title of the chapter (e.g., â€œSi Padre Damasoâ€)

chapter_text â†’ the buod (summary) of the chapter

Part 3 â€“ Code Requirements

Code 1 â€“ Training (CSV 1 + CSV 2 Integrated, on Colab GPU)

Provide Python code that:

Loads CSV 1 (archaic words) and builds a mapping for archaic â†’ modern â†’ English.

Loads CSV 2 (chapters).

Normalizes the chapter text using CSV 1 mapping.

Uses BERTopic with a Tagalog-compatible embedding model (e.g., jcblaise/bert-tagalog-base-cased or XLM-R).

Runs efficiently on Google Colab GPU.

Uses a reasonable number of epochs for fine-tuning (recommend 3â€“5).

Saves the trained BERTopic model for later querying.

Code 2 â€“ Terminal User Testing Script

Provide Python code that:

Loads the trained BERTopic model.

Lets the user type a query in terminal (Tagalog or English).

Processes the query directly through the trained model (no CSV 1 lookup at runtime).

Retrieves and ranks relevant chapters from the trained model.

Generates a short explanatory paragraph under each result explaining the thematic connection.

Works offline only.

Part 4 â€“ Expected Output Format

Please output your response in this format:

ğŸ–¥ï¸ Code 1 â€“ Training (Deep Words + Corpus, Colab GPU, recommended epochs)

ğŸ–¥ï¸ Code 2 â€“ Terminal User Testing Script

âœ… Final Notes & Tips

Make the codes detailed but practical for undergraduate thesis students with intermediate Python and NLP knowledge.